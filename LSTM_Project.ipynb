{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abc1521",
   "metadata": {},
   "source": [
    "# **Data Loading and Preprocessing for LSTM Autoencoder (NAB Dataset)**\n",
    "This notebook covers the **data loading and preprocessing** steps for an LSTM autoencoder anomaly detection project using the **Numenta Anomaly Benchmark (NAB)** dataset. We will load a selected subset of time series, split them into train/validation/test sets, normalize the values, and create sequential windows of data (look-back sequences) suitable as input for an LSTM model. All steps are explained with clear comments for clarity and reproducibility.\n",
    "### **1. Setup and Data Overview**\n",
    "First, let's import the necessary libraries and define the dataset location. We assume the NAB dataset CSV files are available locally in a directory (e.g. nab_data/data). The target time series we will use are:\n",
    "- **machine_temperature_system_failure** – a machine's internal temperature (with system failures).\n",
    "- **ambient_temperature_system_failure** – ambient office temperature (with a system failure event).\n",
    "- **cpu_utilization_asg_misconfiguration** – CPU usage of an AWS cluster (with a misconfiguration anomaly).\n",
    "- **speed_t4013** – traffic speed from sensor T4013.\n",
    "- **speed_7578** – traffic speed from sensor 7578.\n",
    "- **art_daily_jumpsdown** – an artificially generated daily pattern with a sudden jump anomaly.\n",
    "We'll load each CSV into a Pandas DataFrame, parse the timestamps as datetimes, and set them as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d49f65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_temperature_system_failure: loaded 22695 rows, from 2013-12-02 21:15:00 to 2014-02-19 15:25:00\n",
      "ambient_temperature_system_failure: loaded 7267 rows, from 2013-07-04 00:00:00 to 2014-05-28 15:00:00\n",
      "cpu_utilization_asg_misconfiguration: loaded 18050 rows, from 2014-05-14 01:14:00 to 2014-07-15 17:19:00\n",
      "speed_t4013: loaded 2495 rows, from 2015-09-01 11:25:00 to 2015-09-17 16:19:00\n",
      "speed_7578: loaded 1127 rows, from 2015-09-08 11:39:00 to 2015-09-17 14:05:00\n",
      "art_daily_jumpsdown: loaded 4032 rows, from 2014-04-01 00:00:00 to 2014-04-14 23:55:00\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the data directory (adjust the path as needed for your environment)\n",
    "data_dir = Path(\"nab_data/data\")\n",
    "\n",
    "# Define the file paths for each selected time series\n",
    "series_files = {\n",
    "    \"machine_temperature_system_failure\": data_dir / \"realKnownCause/machine_temperature_system_failure.csv\",\n",
    "    \"ambient_temperature_system_failure\": data_dir / \"realKnownCause/ambient_temperature_system_failure.csv\",\n",
    "    \"cpu_utilization_asg_misconfiguration\": data_dir / \"realKnownCause/cpu_utilization_asg_misconfiguration.csv\",\n",
    "    \"speed_t4013\": data_dir / \"realTraffic/speed_t4013.csv\",\n",
    "    \"speed_7578\": data_dir / \"realTraffic/speed_7578.csv\",\n",
    "    \"art_daily_jumpsdown\": data_dir / \"artificialWithAnomaly/art_daily_jumpsdown.csv\"\n",
    "}\n",
    "\n",
    "# Load each series into a DataFrame\n",
    "series_dfs = {}\n",
    "for name, filepath in series_files.items():\n",
    "    # Read CSV, parse dates, set timestamp as index\n",
    "    df = pd.read_csv(filepath, parse_dates=['timestamp'], index_col='timestamp')\n",
    "    df = df.sort_index()  # Ensure the data is sorted by time\n",
    "    series_dfs[name] = df\n",
    "    # Print basic info for verification\n",
    "    print(f\"{name}: loaded {df.shape[0]} rows, from {df.index.min()} to {df.index.max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aba255",
   "metadata": {},
   "source": [
    "Output: The code above will output the number of data points and the date range for each loaded series, confirming that the data has been loaded correctly and is indexed by timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c393fc9",
   "metadata": {},
   "source": [
    "### **2. Train-Validation-Test Split**\n",
    "For each time series, we will split the data chronologically into three segments:\n",
    "- **Training set**: 60% of the earliest data points (used for model training).\n",
    "- **Validation set**: the next 20% of data (used for hyperparameter tuning and model validation).\n",
    "- **Test set**: the final 20% of data (used to evaluate model performance on unseen data).\n",
    "\n",
    "Splitting by time (instead of random splitting) is crucial for time series to respect the chronological order and avoid future data leakage into training. \n",
    "We'll calculate the index boundaries for 60/20/20 split based on the number of samples and then slice the DataFrame accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795e57e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_temperature_system_failure: train 13617, val 4539, test 4539\n",
      "ambient_temperature_system_failure: train 4360, val 1453, test 1454\n",
      "cpu_utilization_asg_misconfiguration: train 10830, val 3610, test 3610\n",
      "speed_t4013: train 1497, val 499, test 499\n",
      "speed_7578: train 676, val 225, test 226\n",
      "art_daily_jumpsdown: train 2419, val 806, test 807\n"
     ]
    }
   ],
   "source": [
    "# Define split ratios\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2  # (test will implicitly be 0.2 as well since train+val+test = 1.0)\n",
    "\n",
    "# Initialize dictionaries to hold split data\n",
    "train_dfs = {}\n",
    "val_dfs = {}\n",
    "test_dfs = {}\n",
    "\n",
    "# Perform chronological splitting for each series\n",
    "for name, df in series_dfs.items():\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_ratio)              # index for end of train set\n",
    "    val_end = train_end + int(n * val_ratio)      # index for end of val set (train_end + 20% of total)\n",
    "    # Slice the DataFrame into train, val, test segments\n",
    "    train_dfs[name] = df.iloc[:train_end]\n",
    "    val_dfs[name]   = df.iloc[train_end:val_end]\n",
    "    test_dfs[name]  = df.iloc[val_end:]\n",
    "    # Verify the split sizes\n",
    "    print(f\"{name}: train {len(train_dfs[name])}, val {len(val_dfs[name])}, test {len(test_dfs[name])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c88b1d",
   "metadata": {},
   "source": [
    "Each series is now split into three sets. The printout confirms the number of points in each split (which should roughly follow a 60%/20%/20% division of the data). We used index slicing (df.iloc[...]) assuming the DataFrame is time-sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe803d",
   "metadata": {},
   "source": [
    "### **3. Feature Scaling with MinMaxScaler**\n",
    "Time series values can have different scales and units. To help the LSTM autoencoder train effectively, we will **normalize** each series using a MinMaxScaler (scaling values to the range [0, 1]). Importantly, the scaler is **fit on the training data only** to avoid leaking information from the validation/test sets. We then transform the validation and test sets using the same scaler parameters (min and max from train). This preserves the relative scale and ensures that anomalies in val/test (which might be out of the train range) are not introduced into the scaling calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6f9217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_temperature_system_failure: train min 0.00, max 1.00\n",
      "ambient_temperature_system_failure: train min 0.00, max 1.00\n",
      "cpu_utilization_asg_misconfiguration: train min 0.00, max 1.00\n",
      "speed_t4013: train min 0.00, max 1.00\n",
      "speed_7578: train min 0.00, max 1.00\n",
      "art_daily_jumpsdown: train min 0.00, max 1.00\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to hold scalers and scaled data\n",
    "scalers = {}\n",
    "scaled_train = {}\n",
    "scaled_val = {}\n",
    "scaled_test = {}\n",
    "\n",
    "for name in series_dfs.keys():\n",
    "    # Initialize a MinMaxScaler for each series\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Fit on training data (expects 2D array)\n",
    "    train_values = train_dfs[name][['value']].values  # shape (n_train, 1)\n",
    "    scaler.fit(train_values)\n",
    "    # Transform train, val, and test data using the fitted scaler\n",
    "    train_scaled = scaler.transform(train_values)\n",
    "    val_scaled   = scaler.transform(val_dfs[name][['value']].values)\n",
    "    test_scaled  = scaler.transform(test_dfs[name][['value']].values)\n",
    "    # Store the scaler and scaled data\n",
    "    scalers[name] = scaler\n",
    "    scaled_train[name] = train_scaled\n",
    "    scaled_val[name]   = val_scaled\n",
    "    scaled_test[name]  = test_scaled\n",
    "    # Optionally, confirm scaling ranges\n",
    "    print(f\"{name}: train min {train_scaled.min():.2f}, max {train_scaled.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe0810",
   "metadata": {},
   "source": [
    "After this step, each series' values are scaled between 0 and 1 (with train set spanning the full [0,1] range by definition of MinMaxScaler). The printed output shows that the min is 0.00 and max is 1.00 for each training set, confirming the scaling. The validation and test values are also now within a 0–1 range (they may exceed 0 or 1 slightly if they have values outside the train range, which can happen if anomalies are present, but that’s acceptable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573350e8",
   "metadata": {},
   "source": [
    "### **4. Creating Overlapping Sequence Windows (Look-back = 288)**\n",
    "**LSTMs are sequence models**, so we need to **convert our scaled data into overlapping sequences of a fixed length**. We choose a **look-back window of 288 time steps** for each sequence. If the data is sampled at 5-minute intervals (as in the NAB dataset for these series), 288 points correspond to 24 hours of data, capturing a full daily cycle. \n",
    "\n",
    "##### **How we create sequences:**\n",
    "- We will use a **sliding window approach**. For a given series segment (train, val, or test), we take the first 288 points as the first sequence, then shift one step forward to get the next sequence (points 2 to 289), and so on.\n",
    "- This yields overlapping sequences of length 288. If a **segment has N points**, this process will **produce N - 288 + 1 sequences**.\n",
    "- For an LSTM autoencoder, we will train the model to **reconstruct the input sequence**. Therefore, we set each sequence as both the input (X) and the target (y) for training. (In other words, y is identical to X for each window in an autoencoder setup.)\n",
    "\n",
    "Let's define a helper function to create these sequences, then apply it to each dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242b88a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_temperature_system_failure: X_train shape (13330, 288, 1), X_val shape (4252, 288, 1), X_test shape (4252, 288, 1)\n",
      "ambient_temperature_system_failure: X_train shape (4073, 288, 1), X_val shape (1166, 288, 1), X_test shape (1167, 288, 1)\n",
      "cpu_utilization_asg_misconfiguration: X_train shape (10543, 288, 1), X_val shape (3323, 288, 1), X_test shape (3323, 288, 1)\n",
      "speed_t4013: X_train shape (1210, 288, 1), X_val shape (212, 288, 1), X_test shape (212, 288, 1)\n",
      "speed_7578: X_train shape (389, 288, 1), X_val shape (0,), X_test shape (0,)\n",
      "art_daily_jumpsdown: X_train shape (2132, 288, 1), X_val shape (519, 288, 1), X_test shape (520, 288, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define look-back window size (e.g., 288 time steps ~ 24 hours of 5-minute data)\n",
    "LOOK_BACK = 288\n",
    "\n",
    "def create_sequences(data_array, window_size):\n",
    "    \"\"\"\n",
    "    Generate overlapping sequences of length `window_size` from a 1D array.\n",
    "    Returns a tuple (X, y) where:\n",
    "      - X is a 3D array of shape (num_sequences, window_size, num_features)\n",
    "      - y is a 3D array of the same shape (for autoencoder target = input sequence)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data_array) - window_size + 1):\n",
    "        seq = data_array[i : i + window_size]\n",
    "        X.append(seq)\n",
    "        y.append(seq)  # for autoencoder, target sequence is the same as input sequence\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# Create sequence windows for each dataset split of each series\n",
    "sequence_data = {}  # to hold the resulting X and y arrays for each series\n",
    "for name in series_dfs.keys():\n",
    "    X_train, y_train = create_sequences(scaled_train[name], LOOK_BACK)\n",
    "    X_val, y_val     = create_sequences(scaled_val[name], LOOK_BACK)\n",
    "    X_test, y_test   = create_sequences(scaled_test[name], LOOK_BACK)\n",
    "    sequence_data[name] = {\n",
    "        \"X_train\": X_train, \"y_train\": y_train,\n",
    "        \"X_val\": X_val,     \"y_val\": y_val,\n",
    "        \"X_test\": X_test,   \"y_test\": y_test\n",
    "    }\n",
    "    print(f\"{name}: X_train shape {X_train.shape}, X_val shape {X_val.shape}, X_test shape {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6e2aa",
   "metadata": {},
   "source": [
    "We now have the input (**X**) and target (**y**) sequences for training, validation, and testing, for each time series. The printed shapes confirm the dimensions: each X (and y) has shape (*number_of_sequences, 288, 1*), since we have 288 time steps and 1 feature per time step. For example, if a training set had 10,000 points, after windowing it would produce 10,000 - 288 + 1 = 9,713 sequences for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc1ec23",
   "metadata": {},
   "source": [
    "### **5. Summary of Prepared Data**\n",
    "At this stage, the data is fully preprocessed and ready for modeling:\n",
    "\n",
    "- **Scaled and windowed data**: For each series, we have X_train, y_train, X_val, y_val, X_test, y_test as NumPy arrays. These can be fed into an LSTM autoencoder model (with X as input and y as target).\n",
    "- **Shape of sequences**: Each sequence is of length 288 with a single feature (the time series value). Thus, X_train.shape is (num_train_sequences, 288, 1). The target y_train has the same shape.\n",
    "- **Next steps**: We would proceed to define the LSTM autoencoder model, train it on the training sequences, validate on the validation set, and use the test set for final anomaly detection performance evaluation. (Those steps will be handled in subsequent notebook sections.)\n",
    "\n",
    "With this preprocessing complete, we have a clean, reproducible pipeline for converting raw NAB time series data into a form suitable for training an LSTM autoencoder for anomaly detection. Each step was carefully executed to avoid data leakage and preserve the time order of events, which is critical in time series anomaly detection tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727af117",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ec958",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
